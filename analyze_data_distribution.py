#!/usr/bin/env python3
"""
Analyze data distribution for SurgViVQA dataset
Outputs: data_distribution.md (for README) and data_stats.json (for app)
"""

import json
from collections import Counter, defaultdict

def analyze_dataset(filepath):
    """Analyze question types and answer distributions"""
    with open(filepath) as f:
        data = [json.loads(line) for line in f]

    # Question type distribution
    qtypes = Counter([s['question_type'] for s in data])

    # Answer type analysis
    answers = [s.get('short_answer', s.get('answer', '')).lower().strip() for s in data]
    yes_no = sum(1 for a in answers if a in ['yes', 'no'])
    yes_count = sum(1 for a in answers if a == 'yes')
    no_count = sum(1 for a in answers if a == 'no')

    # Group by broader categories (optional - for cleaner visualization)
    categories = {
        'Tool & Instrument': ['tool_presence', 'tool_identification', 'tool_count'],
        'Anatomical': ['anatomical_location', 'organ_identification'],
        'Lesion Analysis': ['lesion_presence', 'lesion_type', 'lesion_size', 'lesion_count'],
        'Motion & Dynamics': ['scope_motion', 'scope_forward_motion', 'lesion_motion_direction'],
        'Clinical Assessment': ['occlusion', 'blood_presence', 'phase_identification'],
        'Other': []  # Catch remaining types
    }

    # Map each question type to category
    qtype_to_category = {}
    for cat, qtypes_list in categories.items():
        for qt in qtypes_list:
            qtype_to_category[qt] = cat

    # Assign remaining to "Other"
    for qt in qtypes.keys():
        if qt not in qtype_to_category:
            qtype_to_category[qt] = 'Other'
            categories['Other'].append(qt)

    category_counts = defaultdict(int)
    for qt, count in qtypes.items():
        cat = qtype_to_category[qt]
        category_counts[cat] += count

    return {
        'total_samples': len(data),
        'question_types': dict(qtypes),
        'categories': dict(category_counts),
        'yes_no_breakdown': {
            'yes': yes_count,
            'no': no_count,
            'open_ended': len(data) - yes_no,
            'yes_no_total': yes_no
        }
    }

def generate_markdown_report(train_stats, eval_stats, test_stats):
    """Generate markdown report for README"""

    md = """# Data Distribution

## Dataset Splits

| Split | Samples | Videos | Purpose |
|-------|---------|--------|---------|
| Train | {:,} | 002-001, 002-002, 002-003 | Model training |
| Eval  | {:,} | 002-001, 002-002, 002-003 | Validation during training |
| Test  | {:,} | 002-004 (held-out) | Final evaluation |

## Question Type Distribution (Full Dataset)

""".format(
        train_stats['total_samples'],
        eval_stats['total_samples'],
        test_stats['total_samples']
    )

    # Use training set for distribution (most representative)
    all_samples = train_stats['total_samples'] + eval_stats['total_samples'] + test_stats['total_samples']

    md += "### By Category\n\n"
    md += "| Category | Count | Percentage |\n"
    md += "|----------|-------|------------|\n"

    for cat, count in sorted(train_stats['categories'].items(), key=lambda x: x[1], reverse=True):
        pct = count / train_stats['total_samples'] * 100
        md += f"| {cat} | {count} | {pct:.1f}% |\n"

    md += "\n### Detailed Question Types\n\n"
    md += "| Question Type | Train | Eval | Test |\n"
    md += "|---------------|-------|------|------|\n"

    # Get all unique question types
    all_qtypes = set(train_stats['question_types'].keys()) | \
                 set(eval_stats['question_types'].keys()) | \
                 set(test_stats['question_types'].keys())

    for qtype in sorted(all_qtypes):
        train_count = train_stats['question_types'].get(qtype, 0)
        eval_count = eval_stats['question_types'].get(qtype, 0)
        test_count = test_stats['question_types'].get(qtype, 0)
        md += f"| {qtype} | {train_count} | {eval_count} | {test_count} |\n"

    md += "\n## Answer Type Distribution\n\n"
    md += "| Answer Type | Count | Percentage |\n"
    md += "|-------------|-------|------------|\n"

    # Use training set
    yn = train_stats['yes_no_breakdown']
    total = train_stats['total_samples']

    md += f"| Yes | {yn['yes']} | {yn['yes']/total*100:.1f}% |\n"
    md += f"| No | {yn['no']} | {yn['no']/total*100:.1f}% |\n"
    md += f"| Open-ended | {yn['open_ended']} | {yn['open_ended']/total*100:.1f}% |\n"
    md += f"| **Total** | **{total}** | **100.0%** |\n"

    return md

def generate_json_for_app(eval_stats, test_stats):
    """Generate JSON with performance stats for app display"""
    # This will be enhanced later with actual performance metrics
    # For now, just include the distribution

    return {
        'data_distribution': {
            'eval': eval_stats,
            'test': test_stats
        },
        'performance': {
            'overall': {
                'eval_accuracy': 67.84,
                'test_accuracy': 63.40,
                'baseline': 46.0
            },
            'by_category': {
                # Placeholder - we can fill this from evaluation results
                'Tool & Instrument': {'accuracy': 75.0, 'count': 0},
                'Anatomical': {'accuracy': 70.0, 'count': 0},
                'Lesion Analysis': {'accuracy': 65.0, 'count': 0},
                'Motion & Dynamics': {'accuracy': 45.0, 'count': 0},
                'Clinical Assessment': {'accuracy': 68.0, 'count': 0}
            }
        }
    }

def main():
    print("üìä Analyzing SurgViVQA Data Distribution...")

    # Load all three splits
    train_stats = analyze_dataset('train_multivideo.jsonl')
    eval_stats = analyze_dataset('eval_multivideo.jsonl')
    test_stats = analyze_dataset('test_multivideo.jsonl')

    print(f"‚úì Train: {train_stats['total_samples']} samples")
    print(f"‚úì Eval: {eval_stats['total_samples']} samples")
    print(f"‚úì Test: {test_stats['total_samples']} samples")

    # Generate markdown report for README
    print("\nüìù Generating markdown report...")
    md_report = generate_markdown_report(train_stats, eval_stats, test_stats)

    with open('data_distribution.md', 'w') as f:
        f.write(md_report)
    print("‚úì Saved: data_distribution.md")

    # Generate JSON for app
    print("\nüíæ Generating JSON for app...")
    app_data = generate_json_for_app(eval_stats, test_stats)

    with open('data_stats.json', 'w') as f:
        json.dump(app_data, f, indent=2)
    print("‚úì Saved: data_stats.json")

    # Print summary
    print("\n" + "="*60)
    print("SUMMARY")
    print("="*60)
    print(f"Total samples: {train_stats['total_samples'] + eval_stats['total_samples'] + test_stats['total_samples']}")
    print(f"Unique question types: {len(train_stats['question_types'])}")
    print(f"Yes/No questions: {train_stats['yes_no_breakdown']['yes_no_total']} ({train_stats['yes_no_breakdown']['yes_no_total']/train_stats['total_samples']*100:.1f}%)")
    print("="*60)

    print("\n‚úÖ Done! Use these files:")
    print("  - data_distribution.md ‚Üí Copy sections to README.md")
    print("  - data_stats.json ‚Üí App can load this for stats display")

if __name__ == '__main__':
    main()
